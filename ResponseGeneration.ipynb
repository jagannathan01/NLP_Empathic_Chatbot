{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**RESPONSE GENERATION - CHITCHAT**"
      ],
      "metadata": {
        "id": "JDwcuHUdjE6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg5nc2fi3kaZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "QhMNu5_F69VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "m0EeZ7yB7xUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "i4d7ZMf9_10F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_pickle('chitchat.pkl')\n",
        "\n",
        "text_corpus = dataset['message'].tolist()\n",
        "tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in text_corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
      ],
      "metadata": {
        "id": "Q9ZqZEbQAU4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_response(prompt):\n",
        "    tokenized_query = nltk.word_tokenize(prompt.lower())\n",
        "    top_messages = bm25.get_top_n(tokenized_query, text_corpus, n=5)\n",
        "\n",
        "    responses = []\n",
        "    for message in top_messages:\n",
        "        input_ids = tokenizer.encode(message, return_tensors='pt')\n",
        "        output_ids = torch.cat([input_ids, torch.tensor([[tokenizer.eos_token_id]]).to(torch.int64)], dim=1)\n",
        "\n",
        "        output = model.generate(output_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(output[:, output_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "        responses.append(response)\n",
        "\n",
        "    scores = bm25.get_scores(tokenizer.tokenize(' '.join(responses)))\n",
        "    ranked_responses = [response for _, response in sorted(zip(scores, responses), reverse=True)]\n",
        "    return ranked_responses[0]"
      ],
      "metadata": {
        "id": "JLgFk_4uFj-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"How are you\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuQAoGIRA2WN",
        "outputId": "3750121c-fa5a-469d-8306-7f8cb2153500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  How are you\n",
            "Bot Response :  I'm good, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"How is the weather today ?\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7dTyHI8Bj_-",
        "outputId": "4dd8b37d-a9a1-4ca1-fd1b-247183944254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  How is the weather today ?\n",
            "Bot Response :  It's going well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"I like music\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auhyBaofdO98",
        "outputId": "20230b8e-ad1f-4426-cae8-10eed6882de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  I like music\n",
            "Bot Response :  I'm not sure, I've never really listened to music.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"The food was very good\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtkg50kCdXek",
        "outputId": "7b139000-38bd-428f-cac4-86af5ab0a79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  The food was very good\n",
            "Bot Response :  Thanks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"I want to buy a new macbook\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjtulvzKgDXe",
        "outputId": "e930e08e-228e-4eaf-fdf1-a86e475d0f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  I want to buy a new macbook\n",
            "Bot Response :  I'm sure you'll be able to afford it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"My grandpa passed away\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YQYmIcXhS8G",
        "outputId": "e24224b2-f315-47e2-b2ad-562b0e8c9fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  My grandpa passed away\n",
            "Bot Response :  I'm sorry to hear that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"I need to study a lot\"\n",
        "response = generate_response(input)\n",
        "print(\"Input Text : \", input)\n",
        "print(\"Bot Response : \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ci0Yf9h049",
        "outputId": "7f7bb6df-7c44-4e1d-cec1-c1651bb08eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text :  I need to study a lot\n",
            "Bot Response :  I'm sure you'll get there.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESPONSE GENERATION - EMPATHIC**"
      ],
      "metadata": {
        "id": "WX-zvzMgjU1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "i7bPhVOzl0PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOJ1brCj5RPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "df = pd.read_pickle(\"empathicdatafull.pkl\")\n",
        "df = df[[\"prompt\", \"message\", \"context\"]]\n",
        "\n",
        "models = {}\n",
        "bm25_models = {}\n",
        "embeddings = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkKumHC-v6cw"
      },
      "outputs": [],
      "source": [
        "def train_context_model(context):\n",
        "    context_df = df[df[\"context\"] == context]\n",
        "\n",
        "    prompt_list = context_df[\"prompt\"].tolist()\n",
        "    bm25 = BM25Okapi(prompt_list)\n",
        "\n",
        "    utterance_list = context_df[\"message\"].tolist()\n",
        "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "    embeddings = model.encode(utterance_list, show_progress_bar=True)\n",
        "\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "    bm25_file = f\"models/{context}_model.joblib\"\n",
        "    embeddings_file = f\"models/{context}_embeddings.npy\"\n",
        "\n",
        "    model.save(f\"models/{context}_model\")\n",
        "    np.save(embeddings_file, embeddings)\n",
        "    joblib.dump(bm25, bm25_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17gHiCtx5h-R"
      },
      "outputs": [],
      "source": [
        "for context in ['proud','sad','sentimental','surprised','terrified','trusting']:\n",
        "    print(f\"Training model for context: {context}\")\n",
        "    train_context_model(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCMMVd075cVT"
      },
      "outputs": [],
      "source": [
        "def load_context_model(context):\n",
        "    model_path = f\"models/{context}_model\"\n",
        "    embeddings_path = f\"models/{context}_embeddings.npy\"\n",
        "    bm25_path = f\"models/{context}_model.joblib\"\n",
        "    \n",
        "    model = SentenceTransformer(model_path)\n",
        "    embeddings = np.load(embeddings_path)\n",
        "    bm25 = joblib.load(bm25_path)\n",
        "    \n",
        "    return model, bm25, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx-30RKnymku"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for context in ['proud','sad','sentimental','surprised','terrified','trusting']:\n",
        "    print(f\"Loading model for context: {context}\")\n",
        "    model, bm25, embedding = load_context_model(context)\n",
        "    models[context] = model\n",
        "    bm25_models[context] = bm25\n",
        "    embeddings[context] = embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D68vXwOnv8sw"
      },
      "outputs": [],
      "source": [
        "def get_best_response(query, context):\n",
        "    prompt_list = df[df[\"context\"] == context][\"prompt\"].tolist()\n",
        "    scores = bm25_models[context].get_scores(query)\n",
        "    top_indices = np.argsort(scores)[::-1][:10]\n",
        "    top_prompts = [prompt_list[i] for i in top_indices]\n",
        "\n",
        "    utterance_list = df[df[\"context\"] == context][\"message\"].tolist()\n",
        "    prompt_embeddings = embeddings[context][top_indices]\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    cos_scores = np.dot(prompt_embeddings, query_embedding) / (np.linalg.norm(prompt_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
        "    best_index = np.argmax(cos_scores)\n",
        "    best_response = utterance_list[top_indices[best_index]]\n",
        "\n",
        "    return best_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OFryA7Zv-m3",
        "outputId": "855e7da2-887c-4380-ddd2-6cde6232fc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I am scared of the dark\n",
            "Response : hahaha thats funny why are you still afraid of the dark?\n"
          ]
        }
      ],
      "source": [
        "query = \"I am scared of the dark\"\n",
        "context = \"afraid\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"It was my mistake\"\n",
        "context = \"guilty\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME3O6GhEA1ZY",
        "outputId": "75c53596-b783-4ac8-c165-45a2fa2729aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : It was my mistake\n",
            "Response : Oh that is so scary! What happened after?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"He will surely win tomorrow\"\n",
        "context = \"confident\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0yLn0ekBAjb",
        "outputId": "b738cf69-c705-4c71-ea83-0ef650a8d4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : He will surely win tomorrow\n",
            "Response : I certainly hope so, that would be awesome!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I am really mad at him\"\n",
        "context = \"angry\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8cgCGDbBtvY",
        "outputId": "3c3e8066-4799-45ab-e6e6-de7ba529d436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I am really mad at him\n",
            "Response: Oh jeez, did you ask him why he did that?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I am really looking forward to the match\"\n",
        "context = \"excited\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--VNMm0CiuC",
        "outputId": "44fcc418-9b36-47b0-c84a-9a219966baa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I am really looking forward to the match\n",
            "Response: I just love going, when it gets closer to when we are leaving I look forward to it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"My mother loves me so much\"\n",
        "context = \"caring\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZpqYTSYEJsH",
        "outputId": "fe0365c1-2b47-424c-b43f-14c8899212e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : My mother loves me so much\n",
            "Response: My mom is so great! She is always there for me and her grandchildren.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I miss my family\"\n",
        "context = \"lonely\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INOZHRaTSyrE",
        "outputId": "8d0a685e-03d4-43bd-ba15-88f0458b6332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I miss my family\n",
            "Response : I'm longing for my family's love.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I am scared of exams\"\n",
        "context = \"terrified\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lKl40eRVC4-",
        "outputId": "7eca56a5-d2f6-41aa-a4dd-947562dded25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I am scared of exams\n",
            "Response : That sounds terrifying! Are you okay now?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I really miss those days\"\n",
        "context = \"nostalgic\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNGfRI1uVwhA",
        "outputId": "5f567ba4-3f21-4d63-ae62-abe7b796b00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I really miss those days\n",
            "Response : Enjoy it, your 30s go by really fast. I'm clinging desperately to the last few years of mine myself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I am proud of your efforts\"\n",
        "context = \"proud\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJJ3PwCoWgEg",
        "outputId": "71e2e112-dc66-4fd4-f100-00d3417b0499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I am proud of your efforts\n",
            "Response : That's great that they accomplished that and even better that you recognize their achievements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I trust in you.\"\n",
        "context = \"trusting\"\n",
        "response = get_best_response(query, context)\n",
        "print(f\"Input : {query}\")\n",
        "print(f\"Response : {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TfJkFmXXxHe",
        "outputId": "b6fd7e52-6703-450c-d137-7e35c3f39d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : I trust in you.\n",
            "Response : Yes, I really felt like he had my best interests at heart.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESPONSE GENERATION - REDDIT**"
      ],
      "metadata": {
        "id": "kbK1jj_exkzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T01:15:28.372992Z",
          "iopub.execute_input": "2023-05-15T01:15:28.373530Z",
          "iopub.status.idle": "2023-05-15T01:15:52.987145Z",
          "shell.execute_reply.started": "2023-05-15T01:15:28.373494Z",
          "shell.execute_reply": "2023-05-15T01:15:52.985965Z"
        },
        "trusted": true,
        "id": "-y1ej3hdIPRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T01:15:52.989630Z",
          "iopub.execute_input": "2023-05-15T01:15:52.991217Z",
          "iopub.status.idle": "2023-05-15T01:16:26.642066Z",
          "shell.execute_reply.started": "2023-05-15T01:15:52.991170Z",
          "shell.execute_reply": "2023-05-15T01:16:26.641087Z"
        },
        "trusted": true,
        "id": "9P2dxTa_IPRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_response(prompt, subreddit):\n",
        "    dataset = pd.read_pickle(\"irdata_50k.pkl\")\n",
        "\n",
        "    dataset = dataset[dataset[\"subreddit\"] == subreddit]\n",
        "\n",
        "    text_corpus = dataset['message'].tolist()\n",
        "    tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in text_corpus]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    \n",
        "    tokenized_query = nltk.word_tokenize(prompt.lower())\n",
        "    top_messages = bm25.get_top_n(tokenized_query, text_corpus, n=5)\n",
        "\n",
        "    responses = []\n",
        "    for message in top_messages:\n",
        "        input_ids = tokenizer.encode(message, return_tensors='pt')\n",
        "        output_ids = model.generate(input_ids, max_length=10, num_return_sequences=1, temperature=1.0)\n",
        "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        \n",
        "        if re.search(r'[.!]', response):\n",
        "            response = re.split(r'[.!]', response)[0] + '.'\n",
        "            \n",
        "        responses.append(message)\n",
        "    scores = bm25.get_scores(tokenizer.tokenize(' '.join(responses)))\n",
        "    ranked_responses = [response for _, response in sorted(zip(scores, responses), reverse=True)]\n",
        "\n",
        "    return ranked_responses[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T01:16:26.643643Z",
          "iopub.execute_input": "2023-05-15T01:16:26.644139Z",
          "iopub.status.idle": "2023-05-15T01:16:26.654287Z",
          "shell.execute_reply.started": "2023-05-15T01:16:26.644102Z",
          "shell.execute_reply": "2023-05-15T01:16:26.653382Z"
        },
        "trusted": true,
        "id": "GadWVKiAIPRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Are apple iphones secure?'\n",
        "subreddit = \"technology\"\n",
        "response = generate_response(prompt, subreddit)\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Response: \",response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T02:03:25.375867Z",
          "iopub.execute_input": "2023-05-15T02:03:25.376511Z",
          "iopub.status.idle": "2023-05-15T02:03:34.900473Z",
          "shell.execute_reply.started": "2023-05-15T02:03:25.376476Z",
          "shell.execute_reply": "2023-05-15T02:03:34.899358Z"
        },
        "trusted": true,
        "id": "AyIgsIpJJ5My",
        "outputId": "2296a426-b529-42c1-900c-2adf5239ad4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Are apple iphones secure?\n",
            "Response: Where has Apple ever claimed your iCloud content is secure *from Apple*? I’m quite sure they haven’t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'What do you think about professors'\n",
        "subreddit = \"education\"\n",
        "response = generate_response(prompt, subreddit)\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Response: \",response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T02:04:08.338826Z",
          "iopub.execute_input": "2023-05-15T02:04:08.339504Z",
          "iopub.status.idle": "2023-05-15T02:04:25.297946Z",
          "shell.execute_reply.started": "2023-05-15T02:04:08.339470Z",
          "shell.execute_reply": "2023-05-15T02:04:25.296754Z"
        },
        "trusted": true,
        "id": "KQH8Lz-vJ5My",
        "outputId": "59de6c57-ef4e-4680-c07d-ebbe1e3e61c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What do you think about professors\n",
            "Response: Haha you think college professors make that much more than high school teachers.....  That might be true if they could get a full time position maybe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Talk about medical bills at the hospital'\n",
        "subreddit = \"healthcare\"\n",
        "response = generate_response(prompt, subreddit)\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Response: \",response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T02:04:25.300356Z",
          "iopub.execute_input": "2023-05-15T02:04:25.301015Z",
          "iopub.status.idle": "2023-05-15T02:04:39.377184Z",
          "shell.execute_reply.started": "2023-05-15T02:04:25.300980Z",
          "shell.execute_reply": "2023-05-15T02:04:39.376141Z"
        },
        "trusted": true,
        "id": "7l7ka2rzJ5Mz",
        "outputId": "ba77216f-e95b-4590-efec-dafc55de3715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Talk about medical bills at the hospital\n",
            "Response: It's the same that whether it's the high deductible or the high price of medical bills, I still can't afford it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Donald trump makes a move in elections'\n",
        "subreddit = \"politics\"\n",
        "response = generate_response(prompt, subreddit)\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Response: \",response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T02:04:39.378593Z",
          "iopub.execute_input": "2023-05-15T02:04:39.380205Z",
          "iopub.status.idle": "2023-05-15T02:04:46.401246Z",
          "shell.execute_reply.started": "2023-05-15T02:04:39.380156Z",
          "shell.execute_reply": "2023-05-15T02:04:46.400066Z"
        },
        "trusted": true,
        "id": "gg3OeSrmJ5Mz",
        "outputId": "7f8e0deb-e3bd-4377-ca28-c170794ae933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Donald trump makes a move in elections\n",
            "Response: Its Saturday, almost 5pm EST. Why is Donald Trump still in power?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Talk about planet Earth'\n",
        "subreddit = \"environment\"\n",
        "response = generate_response(prompt, subreddit)\n",
        "print(\"Prompt: \", prompt)\n",
        "print(\"Response: \",response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-15T02:04:46.403576Z",
          "iopub.execute_input": "2023-05-15T02:04:46.403962Z",
          "iopub.status.idle": "2023-05-15T02:04:59.600539Z",
          "shell.execute_reply.started": "2023-05-15T02:04:46.403926Z",
          "shell.execute_reply": "2023-05-15T02:04:59.599473Z"
        },
        "trusted": true,
        "id": "sZ-tOx8fJ5Mz",
        "outputId": "23e13b40-a6a7-498c-aabc-e7a2e84ea4cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Talk about planet Earth\n",
            "Response: After global warming does its worst, the most habitable planet in the solar system will still be Earth.\n"
          ]
        }
      ]
    }
  ]
}