{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CHITCHAT TEST SET**"
      ],
      "metadata": {
        "id": "mNM2d7u44Dth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_pickle('chitchat.pkl') \n",
        "\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=7)\n",
        "\n",
        "with open('chittest.pkl', 'wb') as test_file:\n",
        "    pickle.dump(test_data, test_file)"
      ],
      "metadata": {
        "id": "cXwA-2rZ3W7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COSINE SIMILARITY TO GET GOLDEN RESPONSE - CHITCHAT**"
      ],
      "metadata": {
        "id": "p9-H0qfn2FD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "df = pd.read_pickle('chitchat.pkl')\n",
        "\n",
        "def similar(prompt):\n",
        "  input_sentence = prompt\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  prompt_vectors = vectorizer.fit_transform(df['prompt'])\n",
        "\n",
        "  input_vector = vectorizer.transform([input_sentence])\n",
        "\n",
        "  similarities = cosine_similarity(input_vector, prompt_vectors).flatten()\n",
        "\n",
        "  most_similar_index = np.argmax(similarities)\n",
        "\n",
        "  most_similar_prompt = df.loc[most_similar_index, 'prompt']\n",
        "  most_similar_message = df.loc[most_similar_index, 'message']\n",
        "\n",
        "  return most_similar_message"
      ],
      "metadata": {
        "id": "EwAVqIcN7kFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT SCORE**"
      ],
      "metadata": {
        "id": "LA1VgLPe2Snc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "lan7_hND8kYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "def bertscore(response, goldresponse):\n",
        "  reference_text = goldresponse\n",
        "  generated_text = response\n",
        "\n",
        "  bert_score = score(generated_text, reference_text, lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "  f1_score = bert_score[2][0].item()\n",
        "\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "nAQOzKOo8Ncn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"chittest.pkl\")\n",
        "res = []\n",
        "gold = []\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  res.append(response)\n",
        "  gold.append(goldresponse)"
      ],
      "metadata": {
        "id": "vAeYGf9_sfbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertsc = bertscore(res, gold)\n",
        "bertsc = format(bertsc, \".2f\")\n",
        "print(\"BERT : \",bertsc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUfEHGkn-sZ8",
        "outputId": "f0ed930c-f0a7-404a-fbf3-bd0799b3fb6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT : 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERPLEXITY**"
      ],
      "metadata": {
        "id": "dlaIKEU_2fUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), inputs[\"input_ids\"].view(-1))\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "    return perplexity.item()"
      ],
      "metadata": {
        "id": "AaZx1Zbbt3P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"chittest.pkl\")\n",
        "perpl = 0\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  perpl += calculate_perplexity(response)\n",
        "\n",
        "perpl = perpl/len(data)\n",
        "perpl = format(perpl, \".2f\")\n",
        "print(perpl/len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-bwa38f_3Ww",
        "outputId": "187d5de5-ebb0-4464-ea84-8ad7db1099c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERPLEXITY : 15.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE**"
      ],
      "metadata": {
        "id": "EWjdfPwc2hyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "vXHD7DOwCYNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def rougescore(response, goldresponse):\n",
        "  reference_text = goldresponse\n",
        "  generated_text = response\n",
        "\n",
        "  rouge = Rouge()\n",
        "\n",
        "  scores = rouge.get_scores(generated_text, reference_text)\n",
        "\n",
        "  rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n",
        "  rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n",
        "  rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
        "\n",
        "  return rouge_1_score, rouge_2_score, rouge_l_score"
      ],
      "metadata": {
        "id": "T8RFrTAvAYwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"chittest.pkl\")\n",
        "rougesc1 = 0\n",
        "rougesc2 = 0\n",
        "rougescl = 0\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  x,y,z = rougescore(response, goldresponse)\n",
        "  rougesc1+=x\n",
        "  rougesc2+=y\n",
        "  rougescl+=z\n",
        "\n",
        "rougesc1 = format(rougesc1, \".2f\")\n",
        "rougesc2 = format(rougesc2, \".2f\")\n",
        "rougescl = format(rougescl, \".2f\")\n",
        "\n",
        "print(\"ROUGE 1 : \",rougesc1/len(data))\n",
        "print(\"ROUGE 2 : \",rougesc2/len(data))\n",
        "print(\"ROUGE L : \",rougescl/len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeXpXKTVCQI5",
        "outputId": "878a0eb1-4829-406f-f48a-bca3c5880b05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 : 0.27\n",
            "ROUGE 2 : 0.11\n",
            "ROUGE L : 0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU**"
      ],
      "metadata": {
        "id": "BQmmT_dq2vwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bleuscore(response, goldresponse):\n",
        "\n",
        "  reference_sentence = goldresponse\n",
        "  generated_sentence = response\n",
        "\n",
        "  reference_tokens = reference_sentence.split()\n",
        "  generated_tokens = generated_sentence.split()\n",
        "\n",
        "  bleu_score = sentence_bleu([reference_tokens], generated_tokens, weights=(0.9,0.1))\n",
        "\n",
        "  return bleu_score"
      ],
      "metadata": {
        "id": "mxAbzAecDIo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"chittest.pkl\")\n",
        "bleusc = 0\n",
        "res = []\n",
        "gold = []\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  res.append(response)\n",
        "  gold.append(goldresponse)\n",
        "  bleusc = bleuscore(res, gold)\n",
        "\n",
        "bleusc = format(bleusc, \".2f\")\n",
        "print(\"BLEU : \",bleusc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3c482c-be0f-4315-f670-a512b39a31c3",
        "id": "uvNaZdISNwSw"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU : 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EMPATHIC TEST SET**"
      ],
      "metadata": {
        "id": "7xUFA5zE3yRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_pickle('empathicdatafull.pkl') \n",
        "\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=7)\n",
        "\n",
        "with open('empathictest.pkl', 'wb') as test_file:\n",
        "    pickle.dump(test_data, test_file)"
      ],
      "metadata": {
        "id": "e_yzbXIl3pmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COSINE SIMILARITY TO GET GOLDEN RESPONSE - EMPATHIC**"
      ],
      "metadata": {
        "id": "4MTT2uow3pms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "df = pd.read_pickle('empathicdatafull.pkl')\n",
        "\n",
        "def similar(prompt):\n",
        "  input_sentence = prompt\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  prompt_vectors = vectorizer.fit_transform(df['prompt'])\n",
        "\n",
        "  input_vector = vectorizer.transform([input_sentence])\n",
        "\n",
        "  similarities = cosine_similarity(input_vector, prompt_vectors).flatten()\n",
        "\n",
        "  most_similar_index = np.argmax(similarities)\n",
        "\n",
        "  most_similar_prompt = df.loc[most_similar_index, 'prompt']\n",
        "  most_similar_message = df.loc[most_similar_index, 'message']\n",
        "\n",
        "  return most_similar_message"
      ],
      "metadata": {
        "id": "WwApWBG83pms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT SCORE**"
      ],
      "metadata": {
        "id": "flfsG37_3pmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "Q3ZYdr543pmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "def bertscore(response, goldresponse):\n",
        "  reference_text = goldresponse\n",
        "  generated_text = response\n",
        "\n",
        "  print(len(reference_text))\n",
        "  print(len(generated_text))\n",
        "\n",
        "  bert_score = score(generated_text, reference_text, lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "  f1_score = bert_score[2][0].item()\n",
        "\n",
        "  return f1_score"
      ],
      "metadata": {
        "id": "rYex3aEF3pmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"empathictest.pkl\")\n",
        "res = []\n",
        "gold = []\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  res.append(response)\n",
        "  gold.append(goldresponse)"
      ],
      "metadata": {
        "id": "nONE4uB23pmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertsc = bertscore(res, gold)\n",
        "bertsc = format(bertsc, \".2f\")\n",
        "print(\"BERT : \",bertsc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRI9W1Qa3pmt",
        "outputId": "f0b0105e-8e15-474e-e941-518b5aae0504"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT : 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERPLEXITY**"
      ],
      "metadata": {
        "id": "w0tvXCVL3pmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), inputs[\"input_ids\"].view(-1))\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "    return perplexity.item()"
      ],
      "metadata": {
        "id": "jsGHIulj3pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"empathictest.pkl\")\n",
        "perpl = 0\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  perpl += calculate_perplexity(response)\n",
        "\n",
        "perpl = perpl/len(data)\n",
        "perpl = format(perpl, \".2f\")\n",
        "print(perpl/len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y98y6sL43pm4",
        "outputId": "6b9daf97-9096-411f-990a-f64b39ca5e5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERPLEXITY : 15.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE**"
      ],
      "metadata": {
        "id": "qbCWBhYk3pm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "ltpVJ_cB3pm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def rougescore(response, goldresponse):\n",
        "  reference_text = goldresponse\n",
        "  generated_text = response\n",
        "\n",
        "  rouge = Rouge()\n",
        "\n",
        "  scores = rouge.get_scores(generated_text, reference_text)\n",
        "\n",
        "  rouge_1_score = scores[0][\"rouge-1\"][\"f\"]\n",
        "  rouge_2_score = scores[0][\"rouge-2\"][\"f\"]\n",
        "  rouge_l_score = scores[0][\"rouge-l\"][\"f\"]\n",
        "\n",
        "  return rouge_1_score, rouge_2_score, rouge_l_score"
      ],
      "metadata": {
        "id": "UQxNKVBO3pm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"empathictest.pkl\")\n",
        "rougesc1 = 0\n",
        "rougesc2 = 0\n",
        "rougescll = 0\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  x,y,z = rougescore(response, goldresponse)\n",
        "  rougesc1+=x\n",
        "  rougesc2+=y\n",
        "  rougescll+=z\n",
        "\n",
        "rougesc1 = format(rougesc1, \".2f\")\n",
        "rougesc2 = format(rougesc2, \".2f\")\n",
        "rougescl = format(rougescl, \".2f\")\n",
        "\n",
        "print(\"ROUGE 1 : \",rougesc1/len(data))\n",
        "print(\"ROUGE 2 : \",rougesc2/len(data))\n",
        "print(\"ROUGE L : \",rougescl/len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVF4md703pm5",
        "outputId": "54cab70b-953a-4d21-ddab-d21c684e1b75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 : 0.26\n",
            "ROUGE 2 : 0.07\n",
            "ROUGE L : 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU**"
      ],
      "metadata": {
        "id": "K_Jys3bQ3pm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bleuscore(response, goldresponse):\n",
        "\n",
        "  reference_sentence = goldresponse\n",
        "  generated_sentence = response\n",
        "\n",
        "  reference_tokens = reference_sentence.split()\n",
        "  generated_tokens = generated_sentence.split()\n",
        "\n",
        "  bleu_score = sentence_bleu([reference_tokens], generated_tokens, weights=(0.9,0.1))\n",
        "\n",
        "  return bleu_score"
      ],
      "metadata": {
        "id": "9f3nK_q13pm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle(\"empathictest.pkl\")\n",
        "bleusc = 0\n",
        "res = []\n",
        "gold = []\n",
        "\n",
        "for i,row in data.iterrows():\n",
        "  prompt = row['prompt']\n",
        "  response = generate_response(prompt)\n",
        "  goldresponse = similar(prompt)\n",
        "  res.append(response)\n",
        "  gold.append(goldresponse)\n",
        "  bleusc = bleuscore(res, gold)\n",
        "\n",
        "bleusc = format(bleusc, \".2f\")\n",
        "print(\"BLEU : \",bleusc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QNYZPYa3pm5",
        "outputId": "5ffc4e51-6a3f-4e35-9dcc-f167ef24b0c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU : 0.19\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}